{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_PATH = '/home/leelin/Downloads/2021_4_data'\n",
    "\n",
    "config = {'data_path':'/home/leelin/Downloads/2021_4_data',\n",
    "\t\t  'output_path': '/tmp'\n",
    "\t\t }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class NormalizedData(object):\n",
    "\tdef __init__(self, config):\n",
    "\t\t'''\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param config: dictionary of the config\n",
    "\t\t'''\n",
    "\t\tself.en_train_data_path = os.path.join(\n",
    "\t\t    config['data_path'], 'train/train_en.tsv')\n",
    "\t\tself.tr_train_data_path = os.path.join(\n",
    "\t\t    config['data_path'], 'train/train_tr.tsv')\n",
    "\t\tself.predict_path = os.path.join(config['data_path'], 'to_predict.csv')\n",
    "\t\tself.en_doc_info_path = os.path.join(\n",
    "\t\t    config['data_path'], 'doc_info/en_list_result/')\n",
    "\t\tself.tr_doc_info_path = os.path.join(\n",
    "\t\t    config['data_path'], 'doc_info/tr_list_result/')\n",
    "\n",
    "\t\tself.output_path = config['output_path']\n",
    "\n",
    "\t\tself.part_range = 30000  # num of file num in doc_info 'part-xxxx'\n",
    "\n",
    "\t# def file_line_iter(self, file_path, is_doc_info=False):\n",
    "\t# \t'''\n",
    "\t# \t\tArgs:\n",
    "\t# \t\t\t@param file_path: dir path needed while is_doc_info is True, otherwise\n",
    "\t# \t\t\t\tjust normal file path\n",
    "\t# \t'''\n",
    "\t# \tif is_doc_info == True:\n",
    "\n",
    "\t# \telse:\n",
    "\n",
    "\tdef gen_url2docidx(self, output_dir=None, max_line=100000):\n",
    "\t\t'''\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param output_name: if setted, will save DataFrame to target dir\n",
    "\t\t\t\t@param max_line: max number of rows per DataFrame\n",
    "\t\t\treturn a DataFrame with columns = ['url','file_name','line_idx']\n",
    "\t\t'''\n",
    "\n",
    "\tdef normalize_data(self, doc_info=False, predict_file=False, train_dir=False):\n",
    "\t\t'''\n",
    "\t\t\tArgs:\n",
    "\n",
    "\t\t'''\n",
    "\t\t# ! 此处应该用如何替代\n",
    "\t\treplecment_pool = [\n",
    "\t\t\t\t\t\t['\\x01', ' '],\n",
    "\t\t\t\t\t\t['<br>', ' ']]\n",
    "\n",
    "\t\tdef inplace_modify(path, is_tr=False):\n",
    "\t\t\t# print(\"Normalize file: \", path)\n",
    "\t\t\twith open(path, 'r', encoding='utf8') as fr:\n",
    "\t\t\t\tlines = fr.readlines()\n",
    "\t\t\tfor id in range(len(lines)):\n",
    "\t\t\t\tlines[id] = lines[id].lower()\n",
    "\t\t\t\tfor key in replecment_pool:\n",
    "\t\t\t\t\tlines[id] = lines[id].replace(key[0], key[1])\n",
    "\t\t\t\tif is_tr:\n",
    "\t\t\t\t\tlines[id] = lines[id].replace('ı', 'i') \\\n",
    "                               .replace('ğ', 'g') \\\n",
    "                               .replace('ç', 'c') \\\n",
    "                               .replace('ö', 'o') \\\n",
    "                               .replace('ş', 's') \\\n",
    "                               .replace('ü', 'u')\n",
    "\t\t\t\t# print(lines[id])\n",
    "\t\t\twith open(path, 'w', encoding='utf8') as fr:\n",
    "\t\t\t\tfr.writelines(lines)\n",
    "\n",
    "\t\tif doc_info:\n",
    "\t\t\tfor part_num in range(self.part_range):\n",
    "\t\t\t\tinplace_modify(self.en_doc_info_path+'part-%05d' % part_num)\n",
    "\t\t\t\tinplace_modify(self.tr_doc_info_path+'part-%05d' % part_num, is_tr=True)\n",
    "\t\tif predict_file:\n",
    "\t\t\tinplace_modify(self.predict_path)\n",
    "\t\tif train_dir:\n",
    "\t\t\tinplace_modify(self.en_train_data_path)\n",
    "\t\t\tinplace_modify(self.tr_train_data_path, is_tr=True)\n",
    "\n",
    "\tdef train_test_split(*arrays, test_size=0.33, eval_size=None, shuffle=True, group_size=1):\n",
    "\t\t'''\n",
    "\t\t\tSplite the iterable data on index shape[0]\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param *arrays: pandans DataFrame or numpy array\n",
    "\t\t\t\t@param test_size: test data set size, float\n",
    "\t\t\t\t@param shuffle: use shuffle or not\n",
    "\t\t\t\t@param group_size: mulitple lines as a group (DIGIX-predict be top100)\n",
    "\t\t\tReturn:\n",
    "\t\t\t\t(arrays[i]_train, arrays[i]_test, ...)\n",
    "\t\t'''\n",
    "\t\t# TODO: add gen_eval data\n",
    "\t\tresult = []\n",
    "\n",
    "\t\tfor array in arrays:\n",
    "\t\t\tidx = [(i, i+group_size) for i in range(0, len(array), group_size)]\n",
    "\t\t\tif shuffle:\n",
    "\t\t\t\trandom.shuffle(idx)\n",
    "\t\t\tsplite_idx = math.floor(len(idx) * test_size)\n",
    "\n",
    "\t\t\tif isinstance(array, pd.DataFrame):\n",
    "\t\t\t\ttrain_set = pd.DataFrame()\n",
    "\t\t\t\ttest_set = pd.DataFrame()\n",
    "\t\t\t\tfor idx_range in idx[:splite_idx]:  # test\n",
    "\t\t\t\t\ttest_set = test_set.append(\n",
    "\t\t\t\t\t    array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "\t\t\t\tfor idx_range in idx[splite_idx:]:  # train\n",
    "\t\t\t\t\ttrain_set = train_set.append(\n",
    "\t\t\t\t\t    array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "\t\t\t\tresult.append(train_set)\n",
    "\t\t\t\tresult.append(test_set)\n",
    "\n",
    "\t\t\t# TODO: append method of numpy array\n",
    "\t\t\tif isinstance(array, np.ndarray):\n",
    "\t\t\t\ttrain_set, test_set = [], []\n",
    "\t\t\t\tfor idx_range in idx[:splite_idx]:\n",
    "\t\t\t\t\ttest_set.append(array[idx_range[0]:idx_range[1], ])\n",
    "\t\t\t\tfor idx_range in idx[splite_idx:]:\n",
    "\t\t\t\t\ttrain_set.append(array[idx_range[0]: idx_range[1], ])\n",
    "\t\t\t\tresult.append(train_set)\n",
    "\t\t\t\tresult.append(test_set)\n",
    "\n",
    "\t\treturn tuple(result)\n",
    "\n",
    "\n",
    "    def get_sample(self, queries, query, ranking, sampled_queries, qids):\n",
    "\t\t'''\n",
    "\t\t\tGet context base on (qid, query, ranking) and return as list\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param queries: dataFrame stored predict_data\n",
    "\t\t\t\t@param query: query part of target context\n",
    "\t\t\t\t@param ranking: ranking in query group\n",
    "\t\t\t\t@param sampled_queries: \n",
    "\t\t\t\t@param pids: pid vcorresponding the sampled_queries\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\tSampled line's token list\n",
    "\t\t'''\n",
    "        link_index = queries[(queries.loc[:, 'query'] == query) & (\n",
    "            queries.ranking == ranking)].reset_index().at[0, 'link_index'][1:-1]\n",
    "        qid = qids[sampled_queries.index(query)]\n",
    "        part = link_index.split(',')[0] # name of part_xxxx\n",
    "        row = int(link_index.split(',')[1]) # line num in file\n",
    "\n",
    "        if qid[0:2] == 'en':\n",
    "            with open('data/en_list_result/part-%s' % part.zfill(5), 'r') as fp:\n",
    "                line = fp.readlines()[row]\n",
    "        elif qid[0:2] == 'tr':\n",
    "            with open('data/tr_list_result/part-%s' % part.zfill(5), 'r') as fp:\n",
    "                line = fp.readlines()[row]\n",
    "        else:\n",
    "            print(\"the name of qid Error\\n\")\n",
    "            line = \"Error\"\n",
    "\n",
    "\t\tparts = line.split('\\x01')\n",
    "        title = parts[1] * 20  # title has greater weight\n",
    "        content = parts[2]\n",
    "        url = parts[0]\n",
    "        sample = title + content + url\n",
    "        return sample\n",
    "\n",
    "\t\n",
    "    def neg_samples(self, queries, n_tasks):\n",
    "        \"\"\"\n",
    "        随机负采样多个样本\n",
    "\t\tArgs:\n",
    "        \t@param queries: (pd.DataFrame) all of the queries data\n",
    "        \t@param n_tasks: set manualy \n",
    "\n",
    "        return: [], [[]]\n",
    "        \"\"\"\n",
    "        # adding\n",
    "        sampled_content = []\n",
    "        qids = random.sample(set(queries.qid), n_tasks)\n",
    "        sampled_queries = [queries[(queries.qid == qid) & (queries.ranking == 0)\n",
    "\t\t\t\t].reset_index().at[0, 'query'] for qid in qids]\n",
    "\n",
    "        for query in sampled_queries:\n",
    "            # positive sample\n",
    "            pos_sample = self.get_sample(\n",
    "                queries, query, 0, sampled_queries, qids)\n",
    "            # negative sample\n",
    "            neg_sample = self.get_sample(\n",
    "                queries, query, 10, sampled_queries, qids)\n",
    "\n",
    "            sampled_content.append([pos_sample, neg_sample])\n",
    "        assert len(sampled_queries) == len(sampled_content)\n",
    "        return sampled_queries, sampled_content\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = NormalizedData({'data_path': DATA_PATH})\n",
    "data.normalize_data(doc_info=True, predict_file=True, train_dir=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def my_iter(objs):\n",
    "\tfor obj in objs:\n",
    "\t\tcnt = 0\n",
    "\t\tfor item in obj:\n",
    "\t\t\tif cnt >= len(obj):\n",
    "\t\t\t\tbreak\n",
    "\t\t\tyield item\n",
    "\t\t\tcnt += 1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in my_iter([['1', '2'], ['a', 'b']]):\n",
    "\tprint(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "x = np.arange(27).reshape((3, 3, 3))\n",
    "print(x[0:2, ])\n",
    "print(isinstance(x, np.ndarray))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]]\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('py36': conda)"
  },
  "interpreter": {
   "hash": "985d0649a24e43bb4df0ae37860e6629431958360b6414be599d15809dab9428"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}