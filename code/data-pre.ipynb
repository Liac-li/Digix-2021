{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_PATH = '/home/leelin/Downloads/2021_4_data'\n",
    "\n",
    "config = {'data_path':'/home/leelin/Downloads/2021_4_data',\n",
    "\t\t  'output_path': '/tmp'\n",
    "\t\t }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "class NormalizedData(object):\n",
    "\tdef __init__(self, config):\n",
    "\t\t'''\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param config: dictionary of the config\n",
    "\t\t'''\n",
    "\t\tself.en_train_data_path = os.path.join(\n",
    "\t\t    config['data_path'], 'train/train_en.tsv')\n",
    "\t\tself.tr_train_data_path = os.path.join(\n",
    "\t\t    config['data_path'], 'train/train_tr.tsv')\n",
    "\t\tself.predict_path = os.path.join(config['data_path'], 'to_predict.csv')\n",
    "\t\tself.en_doc_info_path = os.path.join(\n",
    "\t\t    config['data_path'], 'doc_info/en_list_result/')\n",
    "\t\tself.tr_doc_info_path = os.path.join(\n",
    "\t\t    config['data_path'], 'doc_info/tr_list_result/')\n",
    "\n",
    "\t\tself.output_path = config['output_path']\n",
    "\n",
    "\t\tself.part_range = 30000  # num of file num in doc_info 'part-xxxx'\n",
    "\n",
    "\t# def file_line_iter(self, file_path, is_doc_info=False):\n",
    "\t# \t'''\n",
    "\t# \t\tArgs:\n",
    "\t# \t\t\t@param file_path: dir path needed while is_doc_info is True, otherwise\n",
    "\t# \t\t\t\tjust normal file path\n",
    "\t# \t'''\n",
    "\t# \tif is_doc_info == True:\n",
    "\n",
    "\t# \telse:\n",
    "\n",
    "\tdef gen_url2docidx(self, output_dir=None, max_line=100000):\n",
    "\t\t'''\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param output_name: if setted, will save DataFrame to target dir\n",
    "\t\t\t\t@param max_line: max number of rows per DataFrame\n",
    "\t\t\treturn a DataFrame with columns = ['url','file_name','line_idx']\n",
    "\t\t'''\n",
    "\n",
    "\tdef normalize_data(self, doc_info=False, predict_file=False, train_dir=False):\n",
    "\t\t'''\n",
    "\t\t\tArgs:\n",
    "\n",
    "\t\t'''\n",
    "\t\t# ! 此处应该用如何替代\n",
    "\t\treplecment_pool = [\n",
    "\t\t\t\t\t\t['\\x01', ' '],\n",
    "\t\t\t\t\t\t['<br>', ' ']]\n",
    "\n",
    "\t\tdef inplace_modify(path, is_tr=False):\n",
    "\t\t\t# print(\"Normalize file: \", path)\n",
    "\t\t\twith open(path, 'r', encoding='utf8') as fr:\n",
    "\t\t\t\tlines = fr.readlines()\n",
    "\t\t\tfor id in range(len(lines)):\n",
    "\t\t\t\tlines[id] = lines[id].lower()\n",
    "\t\t\t\tfor key in replecment_pool:\n",
    "\t\t\t\t\tlines[id] = lines[id].replace(key[0], key[1])\n",
    "\t\t\t\tif is_tr:\n",
    "\t\t\t\t\tlines[id] = lines[id].replace('ı', 'i') \\\n",
    "                               .replace('ğ', 'g') \\\n",
    "                               .replace('ç', 'c') \\\n",
    "                               .replace('ö', 'o') \\\n",
    "                               .replace('ş', 's') \\\n",
    "                               .replace('ü', 'u')\n",
    "\t\t\t\t# print(lines[id])\n",
    "\t\t\twith open(path, 'w', encoding='utf8') as fr:\n",
    "\t\t\t\tfr.writelines(lines)\n",
    "\n",
    "\t\tif doc_info:\n",
    "\t\t\tfor part_num in range(self.part_range):\n",
    "\t\t\t\tinplace_modify(self.en_doc_info_path+'part-%05d' % part_num)\n",
    "\t\t\t\tinplace_modify(self.tr_doc_info_path+'part-%05d' % part_num, is_tr=True)\n",
    "\t\tif predict_file:\n",
    "\t\t\tinplace_modify(self.predict_path)\n",
    "\t\tif train_dir:\n",
    "\t\t\tinplace_modify(self.en_train_data_path)\n",
    "\t\t\tinplace_modify(self.tr_train_data_path, is_tr=True)\n",
    "\n",
    "\tdef train_test_split(*arrays, test_size=0.33, eval_size=None, shuffle=True, group_size=1):\n",
    "\t\t'''\n",
    "\t\t\tSplite the iterable data on index shape[0]\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param *arrays: pandans DataFrame or numpy array\n",
    "\t\t\t\t@param test_size: test data set size, float\n",
    "\t\t\t\t@param shuffle: use shuffle or not\n",
    "\t\t\t\t@param group_size: mulitple lines as a group (DIGIX-predict be top100)\n",
    "\t\t\tReturn:\n",
    "\t\t\t\t(arrays[i]_train, arrays[i]_test, ...)\n",
    "\t\t'''\n",
    "\t\t# TODO: add gen_eval data\n",
    "\t\tresult = []\n",
    "\n",
    "\t\tfor array in arrays:\n",
    "\t\t\tidx = [(i, i+group_size) for i in range(0, len(array), group_size)]\n",
    "\t\t\tif shuffle:\n",
    "\t\t\t\trandom.shuffle(idx)\n",
    "\t\t\tsplite_idx = math.floor(len(idx) * test_size)\n",
    "\n",
    "\t\t\tif isinstance(array, pd.DataFrame):\n",
    "\t\t\t\ttrain_set = pd.DataFrame()\n",
    "\t\t\t\ttest_set = pd.DataFrame()\n",
    "\t\t\t\tfor idx_range in idx[:splite_idx]:  # test\n",
    "\t\t\t\t\ttest_set = test_set.append(\n",
    "\t\t\t\t\t    array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "\t\t\t\tfor idx_range in idx[splite_idx:]:  # train\n",
    "\t\t\t\t\ttrain_set = train_set.append(\n",
    "\t\t\t\t\t    array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "\t\t\t\tresult.append(train_set)\n",
    "\t\t\t\tresult.append(test_set)\n",
    "\n",
    "\t\t\t# TODO: append method of numpy array\n",
    "\t\t\tif isinstance(array, np.ndarray):\n",
    "\t\t\t\ttrain_set, test_set = [], []\n",
    "\t\t\t\tfor idx_range in idx[:splite_idx]:\n",
    "\t\t\t\t\ttest_set.append(array[idx_range[0]:idx_range[1], ])\n",
    "\t\t\t\tfor idx_range in idx[splite_idx:]:\n",
    "\t\t\t\t\ttrain_set.append(array[idx_range[0]: idx_range[1], ])\n",
    "\t\t\t\tresult.append(train_set)\n",
    "\t\t\t\tresult.append(test_set)\n",
    "\n",
    "\t\treturn tuple(result)\n",
    "\n",
    "\n",
    "    def get_sample(self, queries, query, ranking, sampled_queries, qids):\n",
    "\t\t'''\n",
    "\t\t\tGet context base on (qid, query, ranking) and return as list\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t@param queries: dataFrame stored predict_data\n",
    "\t\t\t\t@param query: query part of target context\n",
    "\t\t\t\t@param ranking: ranking in query group\n",
    "\t\t\t\t@param sampled_queries: \n",
    "\t\t\t\t@param pids: pid vcorresponding the sampled_queries\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\tSampled line's token list\n",
    "\t\t'''\n",
    "        link_index = queries[(queries.loc[:, 'query'] == query) & (\n",
    "            queries.ranking == ranking)].reset_index().at[0, 'link_index'][1:-1]\n",
    "        qid = qids[sampled_queries.index(query)]\n",
    "        part = link_index.split(',')[0] # name of part_xxxx\n",
    "        row = int(link_index.split(',')[1]) # line num in file\n",
    "\n",
    "        if qid[0:2] == 'en':\n",
    "            with open('data/en_list_result/part-%s' % part.zfill(5), 'r') as fp:\n",
    "                line = fp.readlines()[row]\n",
    "        elif qid[0:2] == 'tr':\n",
    "            with open('data/tr_list_result/part-%s' % part.zfill(5), 'r') as fp:\n",
    "                line = fp.readlines()[row]\n",
    "        else:\n",
    "            print(\"the name of qid Error\\n\")\n",
    "            line = \"Error\"\n",
    "\n",
    "\t\tparts = line.split('\\x01')\n",
    "\t\ttitle = parts[1] * 20  # title has greater weight\n",
    "        content = parts[2]\n",
    "        url = parts[0]\n",
    "        sample = title + content + url\n",
    "        return sample\n",
    "\n",
    "\t\n",
    "    def neg_samples(self, queries, n_tasks):\n",
    "        \"\"\"\n",
    "        随机负采样多个样本\n",
    "\t\tArgs:\n",
    "        \t@param queries: (pd.DataFrame) all of the queries data\n",
    "        \t@param n_tasks: set manualy \n",
    "\n",
    "        return: [], [[]]\n",
    "        \"\"\"\n",
    "        # adding\n",
    "        sampled_content = []\n",
    "        qids = random.sample(set(queries.qid), n_tasks)\n",
    "        sampled_queries = [queries[(queries.qid == qid) & (queries.ranking == 0)\n",
    "\t\t\t\t].reset_index().at[0, 'query'] for qid in qids]\n",
    "\n",
    "        for query in sampled_queries:\n",
    "            # positive sample\n",
    "            pos_sample = self.get_sample(\n",
    "                queries, query, 0, sampled_queries, qids)\n",
    "            # negative sample\n",
    "            neg_sample = self.get_sample(\n",
    "                queries, query, 10, sampled_queries, qids)\n",
    "\n",
    "            sampled_content.append([pos_sample, neg_sample])\n",
    "        assert len(sampled_queries) == len(sampled_content)\n",
    "        return sampled_queries, sampled_content\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = NormalizedData({'data_path': DATA_PATH})\n",
    "data.normalize_data(doc_info=True, predict_file=True, train_dir=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def my_iter(objs):\n",
    "\tfor obj in objs:\n",
    "\t\tcnt = 0\n",
    "\t\tfor item in obj:\n",
    "\t\t\tif cnt >= len(obj):\n",
    "\t\t\t\tbreak\n",
    "\t\t\tyield item\n",
    "\t\t\tcnt += 1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in my_iter([['1', '2'], ['a', 'b']]):\n",
    "\tprint(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "x = np.arange(27).reshape((3, 3, 3))\n",
    "print(x[0:2, ])\n",
    "print(isinstance(x, np.ndarray))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]]\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "'a%05d'%1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'a00001'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "x = pd.DataFrame()\n",
    "x['a'] = [1, 2, 3]\n",
    "x['rank'] = [10, 50, 100]\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y['a'] = [10]\n",
    "y['rank'] = [1000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "x[x['rank'] == 10]['a']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1\n",
       "Name: a, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "x.sample(n=2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   a  rank\n",
       "0  1    10\n",
       "2  3   100"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "for index, i in x.iterrows():\n",
    "\tprint(i, type(i))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a        1\n",
      "rank    10\n",
      "Name: 0, dtype: int64 <class 'pandas.core.series.Series'>\n",
      "a        2\n",
      "rank    50\n",
      "Name: 1, dtype: int64 <class 'pandas.core.series.Series'>\n",
      "a         3\n",
      "rank    100\n",
      "Name: 2, dtype: int64 <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "y.append(x)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    a  rank\n",
       "0  10  1000\n",
       "0   1    10\n",
       "1   2    50\n",
       "2   3   100"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "x = eval('(1, 2)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "x[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ll = [1, 2, 3]\n",
    "isinstance(ll, list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "text = '12312'\n",
    "not isinstance(text, str)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "a = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "if a:\n",
    "\tprint('1', a)\n",
    "if a is None:\n",
    "\tprint('2', a)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import math\n",
    "\n",
    "math.ceil(1 * 0.3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def train_test_split(*arrays, df_col_name=None, test_size=0.33, eval_size=None, shuffle=True, group_size=1):\n",
    "    \"\"\"\n",
    "        Split data to train, eval, test set\n",
    "        Args:\n",
    "        @param test_size: float, \n",
    "        @param eval_size: default same as test_size\n",
    "        @param shuffle: determine whether to shuffle\n",
    "        @group_size: \n",
    "        return:\n",
    "        array[i]-train, array[i]-eval, array[i]-test\n",
    "    \"\"\"\n",
    "    def get_idx(df, target_col):\n",
    "        res = []\n",
    "        last_point = 0\n",
    "        last_key_value = None\n",
    "        for index, row in df.iterrows():\n",
    "            if last_key_value is None:\n",
    "                last_key_value = row[target_col]\n",
    "            elif last_key_value != row[target_col]:\n",
    "                res.append((last_point, index))\n",
    "                last_point = index\n",
    "                last_key_value = row[target_col]\n",
    "        return res\n",
    "    \n",
    "    result = []\n",
    "    if eval_size is None:\n",
    "        eval_size = test_size\n",
    "\n",
    "    for array in arrays:\n",
    "        if isinstance(array, pd.DataFrame):\n",
    "            idx = get_idx(array, df_col_name)\n",
    "        else:\n",
    "            idx = [(i, i+group_size) for i in range(0, len(array), group_size)]\n",
    "        if shuffle:\n",
    "            random.shuffle(idx)\n",
    "        splite_idx = math.floor(len(idx) * test_size)\n",
    "        # for eval set\n",
    "        train_idx = idx[splite_idx:]\n",
    "        eval_idx = random.sample(train_idx, math.floor(len(train_idx)*eval_size))\n",
    "        eval_num = math.ceil(group_size * eval_size) # ! should be enough for sampling\n",
    "\n",
    "        if isinstance(array, pd.DataFrame):\n",
    "            train_set = pd.DataFrame()\n",
    "            test_set = pd.DataFrame()\n",
    "            eval_set = pd.DataFrame()\n",
    "\n",
    "            for idx_range in idx[:splite_idx]:\n",
    "                test_set = test_set.append(\n",
    "                    array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "            for idx_range in idx[splite_idx:]:  # train\n",
    "                train_set = train_set.append(\n",
    "                    array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "            for idx_range in eval_idx:\n",
    "                eval_set = eval_set.append(\n",
    "                    array.iloc[idx_range[0]:idx_range[1], :].sample(n=eval_num), ignore_index=True)\n",
    "\n",
    "        # TODO: append method of numpy array\n",
    "        if isinstance(array, np.ndarray):\n",
    "            train_set, test_set, eval_set = [], [], []\n",
    "            for idx_range in idx[:splite_idx]:\n",
    "                test_set.append(array[idx_range[0]:idx_range[1], ])\n",
    "                \n",
    "            for idx_range in idx[splite_idx:]:\n",
    "                train_set.append(array[idx_range[0]: idx_range[1], ])\n",
    "            for idx_range in eval_idx:\n",
    "                train_set.append(array[idx_range[0]: idx_range[1], ])\n",
    "              \n",
    "           # get eval_set\n",
    "       \n",
    "\n",
    "\n",
    "    result.append(train_set)\n",
    "    result.append(eval_set)\n",
    "    result.append(test_set)\n",
    "\n",
    "    return tuple(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "x = pd.DataFrame()\n",
    "x['a'] = [i // 10 for i in range(1000)]\n",
    "x['b'] = ['get%05d'%i for i in range(1000)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "5 // 10"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "xtrain, xeval, x_test = train_test_split(x, df_col_name='a')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "print(xtrain)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      a         b\n",
      "0    89  get00890\n",
      "1    89  get00891\n",
      "2    89  get00892\n",
      "3    89  get00893\n",
      "4    89  get00894\n",
      "..   ..       ...\n",
      "665  18  get00185\n",
      "666  18  get00186\n",
      "667  18  get00187\n",
      "668  18  get00188\n",
      "669  18  get00189\n",
      "\n",
      "[670 rows x 2 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# from bert import tokenization\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "turkStem = TurkishStemmer()\n",
    "\n",
    "class TrainData(object):\n",
    "    def __init__(self, config):\n",
    "        self.__vocab_path = os.path.join(\n",
    "            config[\"bert_model_path\"], \"vocab.txt\")\n",
    "        self.__output_path = config[\"output_path\"]\n",
    "        if not os.path.exists(self.__output_path):\n",
    "            os.makedirs(self.__output_path)\n",
    "        self._sequence_length = config[\"sequence_length\"]  # 每条输入的序列处理为定长\n",
    "        self._batch_size = config[\"batch_size\"]\n",
    "\n",
    "        self.__num_samples = config[\"num_samples\"]\n",
    "\n",
    "        self.count = 0 # sample num\n",
    "\n",
    "        self.en_train_data_path = os.path.join(config['data_path'], 'train/train_en.tsv')\n",
    "        self.tr_train_data_path = os.path.join(config['data_path'], 'train/train_tr.tsv')\n",
    "\n",
    "        self.predict_path = os.path.join(config['data_path'], 'to_predict.csv')\n",
    "        self.en_doc_info_path = os.path.join(config['data_path'], 'doc_info/en_list_result/')\n",
    "        self.tr_doc_info_path = os.path.join(config['data_path'], 'doc_info/tr_list_result/')\n",
    "\n",
    "        self.part_range = 30000  # num of file num in doc_info 'part-xxxx'\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_data(file_path):\n",
    "        \"\"\"\n",
    "        :return: DataFrame for all query both in English and Turkish\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(file_path).iloc[:, 1:]\n",
    "        data = data[data.link_index != '(-1, -1)'].reset_index(drop = True)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def sentence_process(self, sentence):\n",
    "        replacement_pool = [\n",
    "            ['<br>', ' '],\n",
    "            ['\"', ' '],\n",
    "            ['\\'', ' '],\n",
    "            ['.', ' '],\n",
    "            [',', ' '],\n",
    "            ['?', ' '],\n",
    "            ['\\\\', ' '],\n",
    "            ['`', ' '],\n",
    "            ['=', ' '],\n",
    "            ['$', ' '],\n",
    "            ['/', ' '],\n",
    "            ['*', ' '],\n",
    "            [';', ' '],\n",
    "            ['-', ' '],\n",
    "            ['^', ' '],\n",
    "            ['|', ' '],\n",
    "            ['%', ' '],\n",
    "            ['\\/', ' '],\n",
    "        ]\n",
    "        sentence = sentence.lower()\n",
    "        for rule in replacement_pool:\n",
    "            sentence = sentence.replace(rule[0], rule[1])\n",
    "        tokens = sentence.split()\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def getWordsFromURL(self, url, lang):\n",
    "        words_list = re.compile(r'[\\:/?=\\-&.,_@%!$0123456789()&*+\\[\\]]+',re.UNICODE).split(url)\n",
    "        drop_words = set(['', 'http', 'https', 'www', 'com', '\\t', 'm', 'b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'n',\n",
    "                      'o', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z'])\n",
    "\n",
    "        return [turkStem.stem(word.lower()) for word in words_list if word.lower() not in drop_words]\n",
    "\n",
    "\n",
    "    def get_tokens(self, series):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                @param series: pandas.core.series.Series with lind_index(part_num, line_num)\n",
    "        \"\"\"\n",
    "        link_index = eval(series['link_index'])\n",
    "        lang = series['qid'][:2]\n",
    "        if lang == 'en':\n",
    "            with open(self.en_doc_info_path+'part-%05d'%link_index[0], 'r', encoding='utf8') as fr:\n",
    "                line = fr.readlines()[link_index[1]]\n",
    "        elif lang == 'tr':\n",
    "            with open(self.tr_doc_info_path+'part-%05d'%link_index[0], 'r', encoding='utf8') as fr:\n",
    "                line = fr.readlines()[link_index[1]]\n",
    "        else:\n",
    "            raise NameError('Language type not match')\n",
    "        \n",
    "        parts = line.split('\\x01')[:3] # ! set as url-title-rank, may need to change\n",
    "        if len(parts) == 1:\n",
    "            title_with_content = ''\n",
    "        elif len(parts) == 2:\n",
    "            title_with_content = parts[1]\n",
    "        else:\n",
    "            title_with_content = (parts[1] + ' ') * 20 + '.' + parts[2]\n",
    "        # print(len(title_with_content), type(title_with_content))\n",
    "        res = self.sentence_process(title_with_content) + 20 * self.getWordsFromURL(parts[0], lang)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "    def neg_samples(self, queries, n_tasks):\n",
    "        \"\"\"\n",
    "        随机负采样多个样本\n",
    "\t\tArgs:\n",
    "        \t@param queries: (pd.DataFrame) all of the queries data\n",
    "        \t@param n_tasks: set manually \n",
    "\n",
    "        return: [], [[]]\n",
    "        \"\"\"\n",
    "        \n",
    "        # adding\n",
    "        # sampled_content = []\n",
    "        # qids = random.sample(set(queries.qid), n_tasks)\n",
    "        # sampled_queries = [queries[(queries.qid == qid) & (\n",
    "        #     queries.ranking == 0)].reset_index().at[0, 'query'] for qid in qids]\n",
    "\n",
    "        # for query in sampled_queries:\n",
    "        #     # positive sample\n",
    "        #     pos_sample = self.get_sample(\n",
    "        #         queries, query, 0, sampled_queries, qids)\n",
    "        #     # negative sample\n",
    "\n",
    "        #     if self.__num_sample > 1:\n",
    "        #     neg_sample = self.get_sample(\n",
    "        #         queries, query, 10, sampled_queries, qids)\n",
    "\n",
    "        #     sampled_content.append([pos_sample, neg_sample])\n",
    "        # assert len(sampled_queries) == len(sampled_content)\n",
    "        # return sampled_queries, sampled_content\n",
    "        \n",
    "        new_queries = []\n",
    "        new_sims = []\n",
    "\n",
    "        qids = random.sample(set(queries.qid), n_tasks)\n",
    "        cor_qids = list(set(queries.qid) - set(qids))\n",
    "\n",
    "\n",
    "        for i in range(n_tasks):\n",
    "            for rank in range(100): \n",
    "                if not queries[(queries['qid'] == qids[i]) & (queries['ranking'] == rank)].empty:\n",
    "                    pos_q = queries[(queries['qid'] == qids[i]) & (queries['ranking'] == rank)] \n",
    "            neg_sim = random.sample(cor_qids, self.__num_samples - 1)\n",
    "            neg_q = queries[queries['qid'].isin(neg_sim)].sample(n=self.__num_samples-1)\n",
    "\n",
    "            tmp = []\n",
    "            for index, item in pos_q.append(neg_q).iterrows():\n",
    "                tmp.append(self.get_tokens(item))\n",
    "\n",
    "            new_queries.append(pos_q['query'])\n",
    "            new_sims.append(tmp)\n",
    "        return new_queries, new_sims\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_test_split(*arrays, df_col_name=None, test_size=0.33, eval_size=None, only_eval=False, shuffle=True, group_size=1):\n",
    "        \"\"\"\n",
    "            Split data to train, eval, test set\n",
    "            Args:\n",
    "            @param test_size: float, \n",
    "            @param eval_size: default same as test_size\n",
    "            @param shuffle: determine whether to shuffle\n",
    "            @group_size: \n",
    "            return:\n",
    "            array[i]-train, array[i]-eval, array[i]-test\n",
    "        \"\"\"\n",
    "        print(\"==\"*20,\"\\n[train_test_split] Begein split train-eval-test\")\n",
    "        def get_idx(df, target_col):\n",
    "            res = []\n",
    "            last_point = 0\n",
    "            last_key_value = None\n",
    "            for index, row in df.iterrows():\n",
    "                if last_key_value is None:\n",
    "                    last_key_value = row[target_col]\n",
    "                elif last_key_value != row[target_col]:\n",
    "                    res.append((last_point, index))\n",
    "                    last_point = index\n",
    "                    last_key_value = row[target_col]\n",
    "                if index % (len(df) // 20) == 0:\n",
    "                    print(\"Get index: {}/{}\".format(index, len(df)))\n",
    "            return res\n",
    "        \n",
    "        result = []\n",
    "        if eval_size is None:\n",
    "            eval_size = test_size\n",
    "    \n",
    "        for array in arrays:\n",
    "            if isinstance(array, pd.DataFrame):\n",
    "                idx = get_idx(array, df_col_name)\n",
    "            else:\n",
    "                idx = [(i, i+group_size) for i in range(0, len(array), group_size)]\n",
    "            if shuffle:\n",
    "                random.shuffle(idx)\n",
    "            # for eval set\n",
    "            if only_eval:\n",
    "                split_idx = 0\n",
    "                train_idx = idx\n",
    "            else:\n",
    "                split_idx = math.floor(len(idx) * test_size)\n",
    "                train_idx = idx[splite_idx:]\n",
    "            eval_idx = random.sample(train_idx, math.floor(len(train_idx)*eval_size))\n",
    "            eval_num = math.ceil(group_size * eval_size)\n",
    "            print(\"[train_test_split] Finish Initializing\")\n",
    "\n",
    "            if isinstance(array, pd.DataFrame):\n",
    "                train_set = pd.DataFrame()\n",
    "                test_set = pd.DataFrame()\n",
    "                eval_set = pd.DataFrame()\n",
    "\n",
    "                if split_idx == 0:\n",
    "                    train_set = copy.copy(array)\n",
    "                    print(\"[train_test_split] Finish Train Set(only eval)\") \n",
    "                else:\n",
    "                    for idx_range in idx[:split_idx]:\n",
    "                        test_set = test_set.append(\n",
    "                            array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "                    print(\"[train_test_split] Get test set\")\n",
    "                    for idx_range in idx[split_idx:]:  # train\n",
    "                        train_set = train_set.append(\n",
    "                            array.iloc[idx_range[0]:idx_range[1], :], ignore_index=True)\n",
    "                    print(\"[train_test_split] Get train set\")\n",
    "                for idx_range in eval_idx:\n",
    "                    eval_set = eval_set.append(\n",
    "                        array.iloc[idx_range[0]:idx_range[1], :].sample(n=eval_num), ignore_index=True)\n",
    "                print(\"[train_test_split] Get eval set\")\n",
    "\n",
    "            if isinstance(array, np.ndarray):\n",
    "                train_set, test_set, eval_set = [], [], []\n",
    "                \n",
    "                if split_idx == 0:\n",
    "                    train_set = copy.copy(array)\n",
    "                    print(\"[tarin_test_split] Finish Train Set(Only eval)\")\n",
    "                else:\n",
    "                    for idx_range in idx[:split_idx]:\n",
    "                        test_set.append(array[idx_range[0]:idx_range[1], ])\n",
    "                    print(\"[train_test_split] Get test set\")\n",
    "                    for idx_range in idx[split_idx:]:\n",
    "                        train_set.append(array[idx_range[0]: idx_range[1], ])\n",
    "                    print(\"[train_test_split] Get train set\")\n",
    "                for idx_range in eval_idx:\n",
    "                    train_set.append(array[idx_range[0]: idx_range[1], ])\n",
    "                print(\"[train_test_split] Get eval set\")\n",
    "                  \n",
    "        result.append(train_set)\n",
    "        result.append(eval_set)\n",
    "        result.append(test_set)\n",
    "\n",
    "        return tuple(result)\n",
    "\n",
    "\n",
    "\n",
    "    def trans_to_index(self, texts):\n",
    "        \"\"\"\n",
    "        将输入转化为索引表示\n",
    "        :param texts: 输入格式：[], 如果is_sim为True，则格式：[[]]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tokenizer = tokenization.FullTokenizer(\n",
    "            vocab_file=self.__vocab_path, do_lower_case=True)\n",
    "        input_ids = []\n",
    "        input_masks = []\n",
    "        segment_ids = []\n",
    "\n",
    "        for text in texts:\n",
    "            if isinstance(text, pd.Series):\n",
    "                tmp = [x for index, x in text.items()]\n",
    "                text = ''.join(tmp)\n",
    "            if isinstance(text, list):\n",
    "                text = ''.join(text)\n",
    "            if not isinstance(text, str):\n",
    "                print(text, type(text))\n",
    "            text = tokenization.convert_to_unicode(text)\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "            # print(tokens)\n",
    "            input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_ids.append(input_id)\n",
    "            input_masks.append([1] * len(input_id))\n",
    "            segment_ids.append([0] * len(input_id))\n",
    "        return input_ids, input_masks, segment_ids\n",
    "\n",
    "\n",
    "    def padding(self, input_ids, input_masks, segment_ids):\n",
    "        \"\"\"\n",
    "        对序列进行补全\n",
    "        :param input_ids:\n",
    "        :param input_masks:\n",
    "        :param segment_ids:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pad_input_ids, pad_input_masks, pad_segment_ids = [], [], []\n",
    "        for input_id, input_mask, segment_id in zip(input_ids, input_masks, segment_ids):\n",
    "            if len(input_id) < self._sequence_length:\n",
    "                pad_input_ids.append(\n",
    "                    input_id + [0] * (self._sequence_length - len(input_id)))\n",
    "                pad_input_masks.append(\n",
    "                    input_mask + [0] * (self._sequence_length - len(input_mask)))\n",
    "                pad_segment_ids.append(\n",
    "                    segment_id + [0] * (self._sequence_length - len(segment_id)))\n",
    "            else:\n",
    "                pad_input_ids.append(input_id[:self._sequence_length])\n",
    "                pad_input_masks.append(input_mask[:self._sequence_length])\n",
    "                pad_segment_ids.append(segment_id[:self._sequence_length])\n",
    "\n",
    "        return pad_input_ids, pad_input_masks, pad_segment_ids\n",
    "\n",
    "\n",
    "    def gen_data(self, file_path):\n",
    "        \"\"\"\n",
    "        生成数据\n",
    "        :param file_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 1，读取原始数据\n",
    "        queries = self.load_data(file_path)\n",
    "        print(\"read finished\")\n",
    "\n",
    "        return queries\n",
    "\n",
    "\n",
    "    def gen_task_samples(self, queries, n_tasks):\n",
    "        \"\"\"\n",
    "        生成训练任务和验证任务\n",
    "        :param queries:\n",
    "        :param n_tasks:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 1, 采样\n",
    "        text_as, text_bs = self.neg_samples(queries, n_tasks)\n",
    "        self.count += 1\n",
    "        print(\"sample {}\".format(self.count))\n",
    "\n",
    "        # 2，输入转索引\n",
    "        input_ids_a, input_masks_a, segment_ids_a = self.trans_to_index(\n",
    "            text_as)\n",
    "        input_ids_a, input_masks_a, segment_ids_a = self.padding(\n",
    "            input_ids_a, input_masks_a, segment_ids_a)\n",
    "\n",
    "        input_ids_b, input_masks_b, segment_ids_b = [], [], []\n",
    "        for text_b in text_bs:\n",
    "            input_id_b, input_mask_b, segment_id_b = self.trans_to_index(\n",
    "                text_b)\n",
    "            input_id_b, input_mask_b, segment_id_b = self.padding(\n",
    "                input_id_b, input_mask_b, segment_id_b)\n",
    "            input_ids_b.append(input_id_b)\n",
    "            input_masks_b.append(input_mask_b)\n",
    "            segment_ids_b.append(segment_id_b)\n",
    "\n",
    "        return input_ids_a, input_masks_a, segment_ids_a, input_ids_b, input_masks_b, segment_ids_b\n",
    "\n",
    "\n",
    "    def next_batch(self, input_ids_a, input_masks_a, segment_ids_a, input_ids_b, input_masks_b, segment_ids_b):\n",
    "        \"\"\"\n",
    "        生成batch数据\n",
    "        :param input_ids_a:\n",
    "        :param input_masks_a:\n",
    "        :param segment_ids_a:\n",
    "        :param input_ids_b:\n",
    "        :param input_masks_b:\n",
    "        :param segment_ids_b:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(\"num of epoch: \", len(input_ids_a))\n",
    "        num_batches = len(input_ids_a) // self._batch_size\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start = i * self._batch_size\n",
    "            end = start + self._batch_size\n",
    "            batch_input_ids_a = input_ids_a[start: end]\n",
    "            batch_input_masks_a = input_masks_a[start: end]\n",
    "            batch_segment_ids_a = segment_ids_a[start: end]\n",
    "\n",
    "            batch_input_ids_b = input_ids_b[start: end]\n",
    "            batch_input_masks_b = input_masks_b[start: end]\n",
    "            batch_segment_ids_b = segment_ids_b[start: end]\n",
    "\n",
    "            yield dict(input_ids_a=batch_input_ids_a,\n",
    "                       input_masks_a=batch_input_masks_a,\n",
    "                       segment_ids_a=batch_segment_ids_a,\n",
    "                       input_ids_b=list(chain(*batch_input_ids_b)),\n",
    "                       input_masks_b=list(chain(*batch_input_masks_b)),\n",
    "                       segment_ids_b=list(chain(*batch_segment_ids_b)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "config = {\n",
    "  \"model_name\": \"ltr_pair\",\n",
    "  \"epochs\": 5,\n",
    "  \"checkpoint_every\": 500,\n",
    "  \"eval_every\": 500,\n",
    "  \"learning_rate\": 2e-5,\n",
    "  \"sequence_length\": 32,\n",
    "  \"batch_size\": 8,\n",
    "  \"num_samples\": 2,\n",
    "  \"train_n_tasks\": 10,\n",
    "  \"eval_n_tasks\": 500,\n",
    "  \"margin\": 0.5,\n",
    "  \"warmup_rate\": 0.1,\n",
    "  \"output_path\": \"output\",\n",
    "  \"bert_model_path\": \"/home/leelin/Project/Pre-trained_models/multi_cased_L-12_H-768_A-12\",\n",
    "  \"data\": \"/home/leelin/Project/DIGIX-2021/digix-2021/data/train_withIndex.csv\",\n",
    "  \"ckpt_model_path\": \"ckpt_model/\",\n",
    "  \"data_path\": \"/home/leelin/Downloads/2021_4_data\"\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "TD = TrainData(config)\n",
    "data = TD.load_data('/home/leelin/Project/DIGIX-2021/digix-2021/data/train_withIndex.csv')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(type(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "d_train, d_eval, d_test = TD.train_test_split(data, df_col_name='query', only_eval=True, eval_size=0.33)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================== \n",
      "[train_test_split] Begein split train-eval-test\n",
      "Get index: 0/591637\n",
      "Get index: 29581/591637\n",
      "Get index: 59162/591637\n",
      "Get index: 88743/591637\n",
      "Get index: 118324/591637\n",
      "Get index: 147905/591637\n",
      "Get index: 177486/591637\n",
      "Get index: 207067/591637\n",
      "Get index: 236648/591637\n",
      "Get index: 266229/591637\n",
      "Get index: 295810/591637\n",
      "Get index: 325391/591637\n",
      "Get index: 354972/591637\n",
      "Get index: 384553/591637\n",
      "Get index: 414134/591637\n",
      "Get index: 443715/591637\n",
      "Get index: 473296/591637\n",
      "Get index: 502877/591637\n",
      "Get index: 532458/591637\n",
      "Get index: 562039/591637\n",
      "Get index: 591620/591637\n",
      "[train_test_split] Finish Initializing\n",
      "[train_test_split] Finish Train Set(only eval)\n",
      "[train_test_split] Get eval set\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "type(d_train)\n",
    "print(d_train.shape, d_eval.shape, d_test.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(591637, 5) (1968, 5) (0, 0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "d_train.to_csv('/home/leelin/Desktop/train.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "d_test.to_csv('/home/leelin/Desktop/test.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "d_eval.to_csv('/home/leelin/Desktop/eval.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "985d0649a24e43bb4df0ae37860e6629431958360b6414be599d15809dab9428"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('py36': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}